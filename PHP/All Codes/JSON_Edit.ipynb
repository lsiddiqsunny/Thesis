{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Messy reformation\n",
    "'''\n",
    "import json\n",
    "import re\n",
    "\n",
    "mydict = {}\n",
    "mylist = []\n",
    "mystr = \"\"\n",
    "mybool = False\n",
    "\n",
    "def messyReform(data):\n",
    "    tlist = []\n",
    "    if type(data) == type(mylist):\n",
    "        for element in data:\n",
    "            newdict = {}\n",
    "            newdict['label'] = 'dummy'\n",
    "            newdict['type'] = 'dummy'\n",
    "            newdict['children'] = messyReform(element)\n",
    "            tlist.append(newdict)\n",
    "    else:\n",
    "        for x in data.keys():\n",
    "            if type(data[x]) != type(mylist) and type(data[x]) != type(mydict):\n",
    "                newdict = {}\n",
    "                newdict['label'] = data[x]\n",
    "                newdict['type'] = x\n",
    "                newdict['children'] = []\n",
    "                tlist.append(newdict)\n",
    "            else:\n",
    "                newdict = {}\n",
    "                newdict['label'] = x\n",
    "                newdict['type'] = x\n",
    "                newdict['children'] = messyReform(data[x])\n",
    "                tlist.append(newdict)\n",
    "    return tlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'afterQuery2.sql', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'afterQuery2.sql', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeParsed1.json', 'beforeParsed2.json', 'beforeQuery1.sql', 'beforeQuery2.sql', 'beforeXML1.xml', 'beforeXML2.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'afterQuery2.sql', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeParsed1.json', 'beforeParsed2.json', 'beforeQuery1.sql', 'beforeQuery2.sql', 'beforeXML1.xml', 'beforeXML2.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeParsed1.json', 'beforeParsed2.json', 'beforeQuery1.sql', 'beforeQuery2.sql', 'beforeXML1.xml', 'beforeXML2.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'afterQuery2.sql', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeParsed1.json', 'beforeParsed2.json', 'beforeQuery1.sql', 'beforeQuery2.sql', 'beforeXML1.xml', 'beforeXML2.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n[['afterQuery1.sql', 'afterQuery2.sql', 'beforeQuery1.sql', 'beforeQuery2.sql']]\n[['afterParsed1.json', 'afterParsed2.json', 'afterQuery1.sql', 'afterQuery2.sql', 'afterXML1.xml', 'afterXML2.xml', 'beforeParsed1.json', 'beforeParsed2.json', 'beforeQuery1.sql', 'beforeQuery2.sql', 'beforeXML1.xml', 'beforeXML2.xml']]\n[['afterQuery1.sql', 'beforeQuery1.sql']]\n[['afterParsed1.json', 'afterQuery1.sql', 'afterXML1.xml', 'beforeParsed1.json', 'beforeQuery1.sql', 'beforeXML1.xml']]\n"
    }
   ],
   "source": [
    "'''\n",
    "Main part\n",
    "Messy reformation\n",
    "'''\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "now = ''\n",
    "nowtree=''\n",
    "# flag=1\n",
    "# if flag==1:\n",
    "#     now='\\\\D\\\\Temp2\\\\'\n",
    "#     nowtree='after_tree'\n",
    "# else:\n",
    "#     now='\\\\Before\\\\Temp2\\\\'\n",
    "#     nowtree='before_tree'\n",
    "\n",
    "base_path = 'D:\\\\Thesis\\\\Undergraduate-Thesis\\\\PHP\\\\All Codes\\\\Dataset\\\\Temp2\\\\'\n",
    "#directories = [files for root, dirs, files in os.walk(base_path)]\n",
    "directories = [x\n",
    "               for x in os.walk(base_path)]\n",
    "for x in directories[0][1]:\n",
    "    filelist = [files for root, dirs, files in os.walk(base_path+x)]\n",
    "    print(filelist)\n",
    "    for y in filelist[0]:\n",
    "        if 'Parsed' in y and '_Parsed' not in y:\n",
    "            if(os.stat(base_path+x+\"\\\\\"+y).st_size == 0):\n",
    "                print(x)\n",
    "                shutil.rmtree(base_path+x)\n",
    "                break\n",
    "            with open(base_path+x+\"\\\\\"+y) as f:\n",
    "                data = json.load(f)\n",
    "            newdict = {}\n",
    "            newdict['label'] = \"root\"\n",
    "            newdict['type'] = \"root\"\n",
    "            newdict['children'] = messyReform(data[\"sqlscript\"][\"statement\"])\n",
    "            with open(base_path+x+\"\\\\_\"+y, 'w') as f:\n",
    "                json.dump(newdict, f)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "Travarce the JSON and Return path to the SQL Query String\n",
    "children - a list\n",
    "repeat - is multiple Query in one file then \n",
    "        repeat = 0 will find first ocurance, \n",
    "        repeat = 1 will find second ocurance\n",
    "'''\n",
    "\n",
    "mystr = \"\"\n",
    "\n",
    "def find_string(children):\n",
    "    path = []           # contains path to the query string\n",
    "    # same shape as \"path\", continuous 1 if connected to ocurance of query, 0 otherwise\n",
    "    marker = []\n",
    "\n",
    "    if len(children) == 0:\n",
    "        return False, path, marker\n",
    "\n",
    "    for i in range(len(children)):\n",
    "        # print(children[i])\n",
    "        if 'label' not in children[i].keys():\n",
    "            continue\n",
    "        if type(children[i]['label']) == type(mystr) and re.search(r'DELETE.*FROM.*|SELECT.*FROM.*|INSERT.*INTO.*|UPDATE.*SET.*', children[i]['label'], re.IGNORECASE) != None:\n",
    "            path.append(i)\n",
    "            marker.append(1)\n",
    "            return True, path, marker\n",
    "        found, nextpath, nextmarker = find_string(children[i]['children'])\n",
    "        if found == True:\n",
    "            path.append(i)\n",
    "            if children[i]['label'] == \"PLUS\" and nextmarker[0] == 1:\n",
    "                marker.append(1)\n",
    "            else:\n",
    "                marker.append(0)\n",
    "            path = path + nextpath\n",
    "            marker = marker + nextmarker\n",
    "            return True, path, marker\n",
    "\n",
    "    return False, path, marker\n",
    "\n",
    "\n",
    "'''\n",
    "Travarce the children and replace \"sqldata\" in place using marker\n",
    "'''\n",
    "\n",
    "\n",
    "def travarce_replace(children, sqldata, path, marker, pos):\n",
    "    if pos >= len(marker):\n",
    "        return children\n",
    "    if marker[pos] == 0:\n",
    "        children[path[pos]]['children'] = travarce_replace(\n",
    "            children[path[pos]]['children'], sqldata, path, marker, pos+1)\n",
    "    else:\n",
    "        children[path[pos]] = sqldata\n",
    "\n",
    "    return children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1\n1 _afterParsed1.json\n10\n10 _afterParsed1.json\n11\n11 _afterParsed1.json\n11 _afterParsed2.json\n12\n12 _afterParsed1.json\n13\n13 _afterParsed1.json\n14\n14 _afterParsed1.json\n14 _afterParsed2.json\n15\n15 _afterParsed1.json\n16\n17\n18\n18 _afterParsed1.json\n19\n2\n2 _afterParsed1.json\n20\n21\n21 _afterParsed1.json\n22\n22 _afterParsed1.json\n23\n23 _afterParsed1.json\n24\n24 _afterParsed1.json\n25\n25 _afterParsed1.json\n26\n26 _afterParsed1.json\n27\n27 _afterParsed1.json\n28\n28 _afterParsed1.json\n28 _afterParsed2.json\n29\n29 _afterParsed1.json\n3\n3 _afterParsed1.json\n30\n30 _afterParsed1.json\n31\n31 _afterParsed1.json\n31 _afterParsed2.json\n32\n32 _afterParsed1.json\n33\n33 _afterParsed1.json\n34\n34 _afterParsed1.json\n34 _afterParsed2.json\n35\n35 _afterParsed1.json\n36\n37\n38\n38 _afterParsed1.json\n39\n4\n4 _afterParsed1.json\n40\n41\n41 _afterParsed1.json\n42\n42 _afterParsed1.json\n43\n43 _afterParsed1.json\n44\n44 _afterParsed1.json\n45\n45 _afterParsed1.json\n46\n46 _afterParsed1.json\n47\n47 _afterParsed1.json\n47 _afterParsed2.json\n48\n48 _afterParsed1.json\n48 _afterParsed2.json\n49\n49 _afterParsed1.json\n5\n5 _afterParsed1.json\n50\n50 _afterParsed1.json\n51\n51 _afterParsed1.json\n51 _afterParsed2.json\n52\n52 _afterParsed1.json\n53\n53 _afterParsed1.json\n54\n54 _afterParsed1.json\n54 _afterParsed2.json\n55\n55 _afterParsed1.json\n56\n57\n58\n58 _afterParsed1.json\n59\n6\n6 _afterParsed1.json\n60\n61\n62\n62 _afterParsed1.json\n63\n63 _afterParsed1.json\n64\n64 _afterParsed1.json\n65\n65 _afterParsed1.json\n66\n66 _afterParsed1.json\n67\n67 _afterParsed1.json\n68\n68 _afterParsed1.json\n68 _afterParsed2.json\n69\n69 _afterParsed1.json\n7\n7 _afterParsed1.json\n70\n70 _afterParsed1.json\n71\n71 _afterParsed1.json\n71 _afterParsed2.json\n72\n72 _afterParsed1.json\n73\n73 _afterParsed1.json\n74\n74 _afterParsed1.json\n74 _afterParsed2.json\n75\n75 _afterParsed1.json\n76\n77\n78\n78 _afterParsed1.json\n79\n8\n8 _afterParsed1.json\n8 _afterParsed2.json\n80\n9\n9 _afterParsed1.json\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ni = 0\\nfor x in directoryList:\\n    with open(x) as r:\\n        sqldata = json.load(r)\\n    _ , path, marker, _ = find_string(children, i++)\\n    children = find_replace(children, sqldata, path, marker, 0)\\n'"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "'''\n",
    "Main part of Replacing SQL parsed datat in Edit Tree\n",
    "Change 'Before' 'After' to parse them\n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "base_path = 'D:\\\\Thesis\\\\Undergraduate-Thesis\\\\PHP\\\\All Codes\\\\Dataset\\\\Temp2\\\\'\n",
    "#directories = [files for root, dirs, files in os.walk(base_path)]\n",
    "directories = [x\n",
    "               for x in os.walk(base_path)]\n",
    "for x in directories[0][1]:\n",
    "    #fileList = []\n",
    "    print(x)\n",
    "    try:\n",
    "        n = int(x)\n",
    "        with open('D:\\\\Thesis\\\\Undergraduate-Thesis\\\\PHP\\\\All Codes\\\\Dataset\\\\'+ str(n) +\"\\\\editTree.json\") as f:\n",
    "            data = json.load(f)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    children = data['after_tree']['children']\n",
    "    filelist = [files for root, dirs, files in os.walk(base_path+now+x)]\n",
    "    for y in filelist[0]:\n",
    "        \n",
    "        if '_afterParsed' in y and os.stat(base_path+now+x+\"\\\\\"+y).st_size > 0:\n",
    "            #fileList.append(y)\n",
    "            print(x, y)\n",
    "            with open(base_path+now+x+\"\\\\\"+y) as r:\n",
    "                sqldata = json.load(r)\n",
    "                \n",
    "            _, path, marker = find_string(children)\n",
    "            children = travarce_replace(children, sqldata, path, marker, 0)\n",
    "            \n",
    "            with open('D:\\\\Thesis\\\\Undergraduate-Thesis\\\\PHP\\\\All Codes\\\\Dataset\\\\'+ str(n) +\"\\\\editTree.json\", 'w') as outfile:\n",
    "                json.dump(data, outfile)\n",
    "    #print(n, x, fileList)\n",
    "    \n",
    "  \n",
    "'''\n",
    "i = 0\n",
    "for x in directoryList:\n",
    "    with open(x) as r:\n",
    "        sqldata = json.load(r)\n",
    "    _ , path, marker, _ = find_string(children, i++)\n",
    "    children = find_replace(children, sqldata, path, marker, 0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}